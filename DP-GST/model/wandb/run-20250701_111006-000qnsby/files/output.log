Logging to ./model/gst_diffusion/000qnsby
/home/sunyahui/anaconda3/envs/difusco/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:479: LightningDeprecationWarning: Setting `Trainer(gpus=[0, 3])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0, 3])` instead.
  f"Setting `Trainer(gpus={gpus!r})` is deprecated in v1.7 and will be removed"
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
----------------------------------------------------------------------------------------------------
GNNEncoder(
  (node_embed): Linear(in_features=256, out_features=256, bias=True)
  (edge_embed): Linear(in_features=256, out_features=256, bias=True)
  (EW_embed): Linear(in_features=256, out_features=256, bias=True)
  (pos_embed): ScalarEmbeddingLinear1D(
    (scalar_proj): Linear(in_features=1, out_features=256, bias=True)
  )
  (edge_pos_embed): ScalarEmbeddingLinear(
    (edge_proj): Linear(in_features=1, out_features=256, bias=True)
  )
  (EW_pos_embed): EWEmbeddingLinear(
    (edge_proj): Linear(in_features=1, out_features=256, bias=True)
  )
  (time_embed): Sequential(
    (0): Linear(in_features=256, out_features=128, bias=True)
    (1): ReLU()
    (2): Linear(in_features=128, out_features=128, bias=True)
  )
  (out): Sequential(
    (0): GroupNorm32(32, 256, eps=1e-05, affine=True)
    (1): ReLU()
    (2): Conv2d(256, 2, kernel_size=(1, 1), stride=(1, 1))
  )
  (layers): ModuleList(
    (0): GNNLayer(
      (U): Linear(in_features=256, out_features=256, bias=True)
      (V): Linear(in_features=256, out_features=256, bias=True)
      (A): Linear(in_features=256, out_features=256, bias=True)
      (B): Linear(in_features=256, out_features=256, bias=True)
      (C): Linear(in_features=256, out_features=256, bias=True)
      (D): Linear(in_features=256, out_features=256, bias=True)
      (U_ew): Linear(in_features=256, out_features=256, bias=True)
      (V_ew): Linear(in_features=256, out_features=256, bias=True)
      (norm_h): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (norm_e): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (norm_ew): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (1): GNNLayer(
      (U): Linear(in_features=256, out_features=256, bias=True)
      (V): Linear(in_features=256, out_features=256, bias=True)
      (A): Linear(in_features=256, out_features=256, bias=True)
      (B): Linear(in_features=256, out_features=256, bias=True)
      (C): Linear(in_features=256, out_features=256, bias=True)
      (D): Linear(in_features=256, out_features=256, bias=True)
      (U_ew): Linear(in_features=256, out_features=256, bias=True)
      (V_ew): Linear(in_features=256, out_features=256, bias=True)
      (norm_h): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (norm_e): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (norm_ew): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (2): GNNLayer(
      (U): Linear(in_features=256, out_features=256, bias=True)
      (V): Linear(in_features=256, out_features=256, bias=True)
      (A): Linear(in_features=256, out_features=256, bias=True)
      (B): Linear(in_features=256, out_features=256, bias=True)
      (C): Linear(in_features=256, out_features=256, bias=True)
      (D): Linear(in_features=256, out_features=256, bias=True)
      (U_ew): Linear(in_features=256, out_features=256, bias=True)
      (V_ew): Linear(in_features=256, out_features=256, bias=True)
      (norm_h): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (norm_e): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (norm_ew): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (3): GNNLayer(
      (U): Linear(in_features=256, out_features=256, bias=True)
      (V): Linear(in_features=256, out_features=256, bias=True)
      (A): Linear(in_features=256, out_features=256, bias=True)
      (B): Linear(in_features=256, out_features=256, bias=True)
      (C): Linear(in_features=256, out_features=256, bias=True)
      (D): Linear(in_features=256, out_features=256, bias=True)
      (U_ew): Linear(in_features=256, out_features=256, bias=True)
      (V_ew): Linear(in_features=256, out_features=256, bias=True)
      (norm_h): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (norm_e): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (norm_ew): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (4): GNNLayer(
      (U): Linear(in_features=256, out_features=256, bias=True)
      (V): Linear(in_features=256, out_features=256, bias=True)
      (A): Linear(in_features=256, out_features=256, bias=True)
      (B): Linear(in_features=256, out_features=256, bias=True)
      (C): Linear(in_features=256, out_features=256, bias=True)
      (D): Linear(in_features=256, out_features=256, bias=True)
      (U_ew): Linear(in_features=256, out_features=256, bias=True)
      (V_ew): Linear(in_features=256, out_features=256, bias=True)
      (norm_h): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (norm_e): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (norm_ew): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (5): GNNLayer(
      (U): Linear(in_features=256, out_features=256, bias=True)
      (V): Linear(in_features=256, out_features=256, bias=True)
      (A): Linear(in_features=256, out_features=256, bias=True)
      (B): Linear(in_features=256, out_features=256, bias=True)
      (C): Linear(in_features=256, out_features=256, bias=True)
      (D): Linear(in_features=256, out_features=256, bias=True)
      (U_ew): Linear(in_features=256, out_features=256, bias=True)
      (V_ew): Linear(in_features=256, out_features=256, bias=True)
      (norm_h): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (norm_e): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (norm_ew): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (6): GNNLayer(
      (U): Linear(in_features=256, out_features=256, bias=True)
      (V): Linear(in_features=256, out_features=256, bias=True)
      (A): Linear(in_features=256, out_features=256, bias=True)
      (B): Linear(in_features=256, out_features=256, bias=True)
      (C): Linear(in_features=256, out_features=256, bias=True)
      (D): Linear(in_features=256, out_features=256, bias=True)
      (U_ew): Linear(in_features=256, out_features=256, bias=True)
      (V_ew): Linear(in_features=256, out_features=256, bias=True)
      (norm_h): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (norm_e): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (norm_ew): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (7): GNNLayer(
      (U): Linear(in_features=256, out_features=256, bias=True)
      (V): Linear(in_features=256, out_features=256, bias=True)
      (A): Linear(in_features=256, out_features=256, bias=True)
      (B): Linear(in_features=256, out_features=256, bias=True)
      (C): Linear(in_features=256, out_features=256, bias=True)
      (D): Linear(in_features=256, out_features=256, bias=True)
      (U_ew): Linear(in_features=256, out_features=256, bias=True)
      (V_ew): Linear(in_features=256, out_features=256, bias=True)
      (norm_h): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (norm_e): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (norm_ew): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
  )
  (time_embed_layers): ModuleList(
    (0): Sequential(
      (0): ReLU()
      (1): Linear(in_features=128, out_features=256, bias=True)
    )
    (1): Sequential(
      (0): ReLU()
      (1): Linear(in_features=128, out_features=256, bias=True)
    )
    (2): Sequential(
      (0): ReLU()
      (1): Linear(in_features=128, out_features=256, bias=True)
    )
    (3): Sequential(
      (0): ReLU()
      (1): Linear(in_features=128, out_features=256, bias=True)
    )
    (4): Sequential(
      (0): ReLU()
      (1): Linear(in_features=128, out_features=256, bias=True)
    )
    (5): Sequential(
      (0): ReLU()
      (1): Linear(in_features=128, out_features=256, bias=True)
    )
    (6): Sequential(
      (0): ReLU()
      (1): Linear(in_features=128, out_features=256, bias=True)
    )
    (7): Sequential(
      (0): ReLU()
      (1): Linear(in_features=128, out_features=256, bias=True)
    )
  )
  (per_layer_out): ModuleList(
    (0): Sequential(
      (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (1): SiLU()
      (2): Linear(in_features=256, out_features=256, bias=True)
    )
    (1): Sequential(
      (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (1): SiLU()
      (2): Linear(in_features=256, out_features=256, bias=True)
    )
    (2): Sequential(
      (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (1): SiLU()
      (2): Linear(in_features=256, out_features=256, bias=True)
    )
    (3): Sequential(
      (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (1): SiLU()
      (2): Linear(in_features=256, out_features=256, bias=True)
    )
    (4): Sequential(
      (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (1): SiLU()
      (2): Linear(in_features=256, out_features=256, bias=True)
    )
    (5): Sequential(
      (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (1): SiLU()
      (2): Linear(in_features=256, out_features=256, bias=True)
    )
    (6): Sequential(
      (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (1): SiLU()
      (2): Linear(in_features=256, out_features=256, bias=True)
    )
    (7): Sequential(
      (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (1): SiLU()
      (2): Linear(in_features=256, out_features=256, bias=True)
    )
  )
)
----------------------------------------------------------------------------------------------------
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 2 processes
----------------------------------------------------------------------------------------------------
You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
Parameters: 5266946
/home/sunyahui/anaconda3/envs/difusco/lib/python3.7/site-packages/torch_geometric/deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Training steps: 120000
  | Name  | Type       | Params
-------------------------------------
0 | model | GNNEncoder | 5.3 M
-------------------------------------
5.3 M     Trainable params
0         Non-trainable params
5.3 M     Total params
21.068    Total estimated model params size (MB)
/home/sunyahui/anaconda3/envs/difusco/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:229: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 64 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  category=PossibleUserWarning,
Sanity Checking: 0it [00:00, ?it/s]Validation dataset size: 64
Sanity Checking DataLoader 0:   0%|                                                                           | 0/2 [00:00<?, ?it/s]error :128.125
Sanity Checking DataLoader 0:  50%|█████████████████████████████████▌                                 | 1/2 [00:53<00:53, 53.36s/it]error :82.62295081967213
Sanity Checking DataLoader 0: 100%|███████████████████████████████████████████████████████████████████| 2/2 [01:42<00:00, 51.00s/it]metrics:{'val_solved_cost_mean': tensor(136.2993, device='cuda:0')}













































































































































































































































































































































































































































































































































































































































































































































































































Epoch 0: 100%|██████████████████████████████████████████████████████▊| 8000/8032 [26:06<00:06,  5.11it/s, loss=8.13e-05, v_num=nsby]
Epoch 0: 100%|██████████████████████████████████████████████████████▊| 8001/8032 [26:08<00:06,  5.10it/s, loss=8.13e-05, v_num=nsby]
Epoch 0: 100%|██████████████████████████████████████████████████████▊| 8002/8032 [26:11<00:05,  5.09it/s, loss=8.13e-05, v_num=nsby]
Epoch 0: 100%|██████████████████████████████████████████████████████▊| 8003/8032 [26:12<00:05,  5.09it/s, loss=8.13e-05, v_num=nsby]
Epoch 0: 100%|██████████████████████████████████████████████████████▊| 8004/8032 [26:14<00:05,  5.08it/s, loss=8.13e-05, v_num=nsby]
Epoch 0: 100%|██████████████████████████████████████████████████████▊| 8005/8032 [26:17<00:05,  5.07it/s, loss=8.13e-05, v_num=nsby]
Epoch 0: 100%|██████████████████████████████████████████████████████▊| 8006/8032 [26:19<00:05,  5.07it/s, loss=8.13e-05, v_num=nsby]
Epoch 0: 100%|██████████████████████████████████████████████████████▊| 8007/8032 [26:21<00:04,  5.06it/s, loss=8.13e-05, v_num=nsby]
Epoch 0: 100%|██████████████████████████████████████████████████████▊| 8008/8032 [26:23<00:04,  5.06it/s, loss=8.13e-05, v_num=nsby]
Epoch 0: 100%|██████████████████████████████████████████████████████▊| 8009/8032 [26:24<00:04,  5.05it/s, loss=8.13e-05, v_num=nsby]
Epoch 0: 100%|██████████████████████████████████████████████████████▊| 8010/8032 [26:26<00:04,  5.05it/s, loss=8.13e-05, v_num=nsby]error :0.0
Epoch 0: 100%|██████████████████████████████████████████████████████▊| 8011/8032 [26:28<00:04,  5.04it/s, loss=8.13e-05, v_num=nsby]
Epoch 0: 100%|██████████████████████████████████████████████████████▊| 8012/8032 [26:29<00:03,  5.04it/s, loss=8.13e-05, v_num=nsby]
Epoch 0: 100%|██████████████████████████████████████████████████████▊| 8013/8032 [26:31<00:03,  5.03it/s, loss=8.13e-05, v_num=nsby]
Epoch 0: 100%|██████████████████████████████████████████████████████▉| 8014/8032 [26:33<00:03,  5.03it/s, loss=8.13e-05, v_num=nsby]
Epoch 0: 100%|██████████████████████████████████████████████████████▉| 8015/8032 [26:35<00:03,  5.02it/s, loss=8.13e-05, v_num=nsby]
Epoch 0: 100%|██████████████████████████████████████████████████████▉| 8016/8032 [26:37<00:03,  5.02it/s, loss=8.13e-05, v_num=nsby]
Epoch 0: 100%|██████████████████████████████████████████████████████▉| 8017/8032 [26:39<00:02,  5.01it/s, loss=8.13e-05, v_num=nsby]
Epoch 0: 100%|██████████████████████████████████████████████████████▉| 8018/8032 [26:41<00:02,  5.01it/s, loss=8.13e-05, v_num=nsby]
Epoch 0: 100%|██████████████████████████████████████████████████████▉| 8019/8032 [26:42<00:02,  5.00it/s, loss=8.13e-05, v_num=nsby]
Epoch 0: 100%|██████████████████████████████████████████████████████▉| 8020/8032 [26:44<00:02,  5.00it/s, loss=8.13e-05, v_num=nsby]
Epoch 0: 100%|██████████████████████████████████████████████████████▉| 8021/8032 [26:46<00:02,  4.99it/s, loss=8.13e-05, v_num=nsby]
Epoch 0: 100%|██████████████████████████████████████████████████████▉| 8022/8032 [26:48<00:02,  4.99it/s, loss=8.13e-05, v_num=nsby]
Epoch 0: 100%|██████████████████████████████████████████████████████▉| 8023/8032 [26:50<00:01,  4.98it/s, loss=8.13e-05, v_num=nsby]
Epoch 0: 100%|██████████████████████████████████████████████████████▉| 8024/8032 [26:52<00:01,  4.98it/s, loss=8.13e-05, v_num=nsby]error :0.0
Epoch 0: 100%|██████████████████████████████████████████████████████▉| 8025/8032 [26:54<00:01,  4.97it/s, loss=8.13e-05, v_num=nsby]
Epoch 0: 100%|██████████████████████████████████████████████████████▉| 8026/8032 [26:55<00:01,  4.97it/s, loss=8.13e-05, v_num=nsby]
Epoch 0: 100%|██████████████████████████████████████████████████████▉| 8027/8032 [26:57<00:01,  4.96it/s, loss=8.13e-05, v_num=nsby]
Epoch 0: 100%|██████████████████████████████████████████████████████▉| 8028/8032 [26:59<00:00,  4.96it/s, loss=8.13e-05, v_num=nsby]
Epoch 0: 100%|██████████████████████████████████████████████████████▉| 8029/8032 [27:01<00:00,  4.95it/s, loss=8.13e-05, v_num=nsby]
Epoch 0: 100%|██████████████████████████████████████████████████████▉| 8030/8032 [27:03<00:00,  4.95it/s, loss=8.13e-05, v_num=nsby]
Epoch 0: 100%|██████████████████████████████████████████████████████▉| 8031/8032 [27:05<00:00,  4.94it/s, loss=8.13e-05, v_num=nsby]
Epoch 0: 100%|███████████████████████████████████████████████████████| 8032/8032 [27:07<00:00,  4.93it/s, loss=8.13e-05, v_num=nsby]metrics:{'val_solved_cost_mean': tensor(0., device='cuda:0')}




































































































































































































































































































































































Epoch 1:  46%|████████████▍              | 3682/8032 [11:53<14:03,  5.16it/s, loss=0.000134, v_num=nsby, val_solved_cost_mean=0.000]
/home/sunyahui/anaconda3/envs/difusco/lib/python3.7/site-packages/pytorch_lightning/trainer/call.py:48: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
  rank_zero_warn("Detected KeyboardInterrupt, attempting graceful shutdown...")